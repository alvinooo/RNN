A lifestyle business is carried out in a way to create a certain level of earning and lifestyle for the owners versus a high-potential business is carried out to create value to its customers. The rewards for the latter are that these grow substantially, they are widely accepted, these solve real problems and give back to the community. This is as opposed to the former in which the former might or might not add great value to its customers and may take shortcuts to make money. The risks and trade-offs in the latter are that in the process of developing a high-potential business, the owners might not get the perfect work-life balance.
Deep neural nets with a large number of parameters are very powerful machine learning
systems. However, overfitting is a serious problem in such networks. Large networks are also
slow to use, making it difficult to deal with overfitting by combining the predictions of many
different large neural nets at test time. Dropout is a technique for addressing this problem.
The key idea is to randomly drop units (along with their connections) from the neural
network during training. This prevents units from co-adapting too much. During training,
dropout samples from an exponential number of different “thinned” networks. At test time,
it is easy to approximate the effect of averaging the predictions of all these thinned networks
by simply using a single unthinned network that has smaller weights. This significantly
reduces overfitting and gives major improvements over other regularization methods. We
show that dropout improves the performance of neural networks on supervised learning
tasks in vision, speech recognition, document classification and computational biology,
obtaining state-of-the-art results on many benchmark data sets.
